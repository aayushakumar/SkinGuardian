{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BEIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BEiT model and feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aayush\\miniconda3\\envs\\qualcomm\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `BeitImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and feature extractor loaded successfully.\n",
      "Modifying model for binary classification...\n",
      "Model adjusted.\n",
      "Loading dataset...\n",
      "Dataset loaded with 2594 unique samples.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BeitForImageClassification, BeitImageProcessor\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, feature_extractor):\n",
    "        print(\"Loading dataset...\")\n",
    "        self.data = pd.read_csv(csv_file).drop_duplicates(subset=['image_path']).reset_index(drop=True)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        print(f\"Dataset loaded with {len(self.data)} unique samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        return inputs['pixel_values'].squeeze(0), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Load pre-trained BEiT model and feature extractor\n",
    "print(\"Loading BEiT model and feature extractor...\")\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\n",
    "feature_extractor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\n",
    "print(\"Model and feature extractor loaded successfully.\")\n",
    "\n",
    "# Enable gradient checkpointing to reduce memory usage\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Adjust model for binary classification\n",
    "print(\"Modifying model for binary classification...\")\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 2)  # 2 classes\n",
    "print(\"Model adjusted.\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset('image_data.csv', feature_extractor)\n",
    "# num_workers=0\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)  # Reduced learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Enable mixed precision training\n",
    "scaler = torch.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/5 - Training started...\n",
      "Epoch 1, Batch 0/163, Loss: 0.8319\n",
      "Epoch 1, Batch 10/163, Loss: 0.5901\n",
      "Epoch 1, Batch 20/163, Loss: 0.4773\n",
      "Epoch 1, Batch 30/163, Loss: 0.1502\n",
      "Epoch 1, Batch 40/163, Loss: 0.5048\n",
      "Epoch 1, Batch 50/163, Loss: 0.6168\n",
      "Epoch 1, Batch 60/163, Loss: 0.4445\n",
      "Epoch 1, Batch 70/163, Loss: 0.2021\n",
      "Epoch 1, Batch 80/163, Loss: 0.5241\n",
      "Epoch 1, Batch 90/163, Loss: 0.6617\n",
      "Epoch 1, Batch 100/163, Loss: 0.1382\n",
      "Epoch 1, Batch 110/163, Loss: 0.4132\n",
      "Epoch 1, Batch 120/163, Loss: 0.2105\n",
      "Epoch 1, Batch 130/163, Loss: 0.2608\n",
      "Epoch 1, Batch 140/163, Loss: 0.4116\n",
      "Epoch 1, Batch 150/163, Loss: 0.6157\n",
      "Epoch 1, Batch 160/163, Loss: 0.5728\n",
      "Epoch 1 completed. Avg Loss: 0.4691, Time taken: 1366.22 sec\n",
      "New best model saved!\n",
      "\n",
      "Epoch 2/5 - Training started...\n",
      "Epoch 2, Batch 0/163, Loss: 0.3528\n",
      "Epoch 2, Batch 10/163, Loss: 0.8584\n",
      "Epoch 2, Batch 20/163, Loss: 0.2477\n",
      "Epoch 2, Batch 30/163, Loss: 0.6037\n",
      "Epoch 2, Batch 40/163, Loss: 0.7081\n",
      "Epoch 2, Batch 50/163, Loss: 0.4866\n",
      "Epoch 2, Batch 60/163, Loss: 0.4557\n",
      "Epoch 2, Batch 70/163, Loss: 0.2691\n",
      "Epoch 2, Batch 80/163, Loss: 0.3427\n",
      "Epoch 2, Batch 90/163, Loss: 0.3652\n",
      "Epoch 2, Batch 100/163, Loss: 0.4105\n",
      "Epoch 2, Batch 110/163, Loss: 0.4884\n",
      "Epoch 2, Batch 120/163, Loss: 0.3628\n",
      "Epoch 2, Batch 130/163, Loss: 0.2929\n",
      "Epoch 2, Batch 140/163, Loss: 0.3231\n",
      "Epoch 2, Batch 150/163, Loss: 0.3042\n",
      "Epoch 2, Batch 160/163, Loss: 0.3407\n",
      "Epoch 2 completed. Avg Loss: 0.3825, Time taken: 1368.62 sec\n",
      "New best model saved!\n",
      "\n",
      "Epoch 3/5 - Training started...\n",
      "Epoch 3, Batch 0/163, Loss: 0.4486\n",
      "Epoch 3, Batch 10/163, Loss: 0.2639\n",
      "Epoch 3, Batch 20/163, Loss: 0.0726\n",
      "Epoch 3, Batch 30/163, Loss: 0.1807\n",
      "Epoch 3, Batch 40/163, Loss: 0.3223\n",
      "Epoch 3, Batch 50/163, Loss: 0.4111\n",
      "Epoch 3, Batch 60/163, Loss: 0.2621\n",
      "Epoch 3, Batch 70/163, Loss: 0.1608\n",
      "Epoch 3, Batch 80/163, Loss: 0.5029\n",
      "Epoch 3, Batch 90/163, Loss: 0.3908\n",
      "Epoch 3, Batch 100/163, Loss: 0.3009\n",
      "Epoch 3, Batch 110/163, Loss: 0.2184\n",
      "Epoch 3, Batch 120/163, Loss: 0.4658\n",
      "Epoch 3, Batch 130/163, Loss: 0.3731\n",
      "Epoch 3, Batch 140/163, Loss: 0.3531\n",
      "Epoch 3, Batch 150/163, Loss: 0.2729\n",
      "Epoch 3, Batch 160/163, Loss: 0.2241\n",
      "Epoch 3 completed. Avg Loss: 0.2976, Time taken: 1365.20 sec\n",
      "New best model saved!\n",
      "\n",
      "Epoch 4/5 - Training started...\n",
      "Epoch 4, Batch 0/163, Loss: 0.1893\n",
      "Epoch 4, Batch 10/163, Loss: 0.1941\n",
      "Epoch 4, Batch 20/163, Loss: 0.1804\n",
      "Epoch 4, Batch 30/163, Loss: 0.1795\n",
      "Epoch 4, Batch 40/163, Loss: 0.1893\n",
      "Epoch 4, Batch 50/163, Loss: 0.5941\n",
      "Epoch 4, Batch 60/163, Loss: 0.1553\n",
      "Epoch 4, Batch 70/163, Loss: 0.2167\n",
      "Epoch 4, Batch 80/163, Loss: 0.2257\n",
      "Epoch 4, Batch 90/163, Loss: 0.2835\n",
      "Epoch 4, Batch 100/163, Loss: 0.1696\n",
      "Epoch 4, Batch 110/163, Loss: 0.0228\n",
      "Epoch 4, Batch 120/163, Loss: 0.1881\n",
      "Epoch 4, Batch 130/163, Loss: 0.2687\n",
      "Epoch 4, Batch 140/163, Loss: 0.7408\n",
      "Epoch 4, Batch 150/163, Loss: 0.2696\n",
      "Epoch 4, Batch 160/163, Loss: 0.0902\n",
      "Epoch 4 completed. Avg Loss: 0.2063, Time taken: 1373.66 sec\n",
      "New best model saved!\n",
      "\n",
      "Epoch 5/5 - Training started...\n",
      "Epoch 5, Batch 0/163, Loss: 0.1583\n",
      "Epoch 5, Batch 10/163, Loss: 0.4800\n",
      "Epoch 5, Batch 20/163, Loss: 0.2180\n",
      "Epoch 5, Batch 30/163, Loss: 0.1982\n",
      "Epoch 5, Batch 40/163, Loss: 0.1833\n",
      "Epoch 5, Batch 50/163, Loss: 0.0082\n",
      "Epoch 5, Batch 60/163, Loss: 0.2212\n",
      "Epoch 5, Batch 70/163, Loss: 0.1754\n",
      "Epoch 5, Batch 80/163, Loss: 0.1550\n",
      "Epoch 5, Batch 90/163, Loss: 0.0611\n",
      "Epoch 5, Batch 100/163, Loss: 0.0159\n",
      "Epoch 5, Batch 110/163, Loss: 0.0599\n",
      "Epoch 5, Batch 120/163, Loss: 0.0665\n",
      "Epoch 5, Batch 130/163, Loss: 0.3203\n",
      "Epoch 5, Batch 140/163, Loss: 0.2874\n",
      "Epoch 5, Batch 150/163, Loss: 0.0039\n",
      "Epoch 5, Batch 160/163, Loss: 0.0337\n",
      "Epoch 5 completed. Avg Loss: 0.1496, Time taken: 1401.87 sec\n",
      "New best model saved!\n",
      "Training completed in 114.63 minutes.\n",
      "Model fine-tuned and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs to wait before stopping if no improvement\n",
    "best_loss = float('inf')\n",
    "stopping_counter = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Training started...\")\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        # if batch_idx % 20 == 0:\n",
    "        #     print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # print(f\"Processing Batch {batch_idx+1}/{len(dataloader)}\")  # Debug print\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast('cuda'):  # Enable mixed precision\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}, Time taken: {(epoch_end_time - epoch_start_time):.2f} sec\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        stopping_counter = 0  # Reset counter if loss improves\n",
    "        model.save_pretrained('fine_tuned_beit_best')  # Save best model\n",
    "        print(\"New best model saved!\")\n",
    "    else:\n",
    "        stopping_counter += 1\n",
    "        print(f\"Early stopping counter: {stopping_counter}/{patience}\")\n",
    "        if stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained('fine_tuned_beit')\n",
    "print(\"Model fine-tuned and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Fine-tuned Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aayush\\miniconda3\\envs\\qualcomm\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:255: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "c:\\Users\\Aayush\\miniconda3\\envs\\qualcomm\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:646: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if interpolate_pos_encoding:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to ONNX!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BeitConfig, BeitForImageClassification\n",
    "import torch\n",
    "\n",
    "# Load the existing config from your fine-tuned checkpoint\n",
    "config = BeitConfig.from_pretrained('fine_tuned_beit')\n",
    "config.num_labels = 2  # explicitly set number of labels\n",
    "\n",
    "# Now load the model using the updated config\n",
    "model = BeitForImageClassification.from_pretrained(\n",
    "    'fine_tuned_beit', \n",
    "    config=config\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Export to ONNX\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    \"beit_finetuned_model.onnx\",\n",
    "    input_names=[\"input\"], \n",
    "    output_names=[\"output\"]\n",
    ")\n",
    "print(\"Model converted to ONNX!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Output logits: [[-2.1029592  1.2445506]]\n",
      "Probabilities: tensor([[0.0340, 0.9660]])\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import BeitImageProcessor\n",
    "\n",
    "# 1. Load the ONNX model into an InferenceSession\n",
    "session = ort.InferenceSession(\"beit_finetuned_model.onnx\")\n",
    "\n",
    "# 2. Create the processor for image preprocessing\n",
    "processor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\n",
    "\n",
    "# 3. Load and preprocess an image\n",
    "image = Image.open(\"classified_images/malignant/ISIC_0000013.jpg\").convert(\"RGB\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")  \n",
    "# inputs[\"pixel_values\"] will be shape [1, 3, 224, 224]\n",
    "\n",
    "# 4. ONNX Runtime expects Numpy arrays as input\n",
    "ort_inputs = {session.get_inputs()[0].name: inputs[\"pixel_values\"].numpy()}\n",
    "\n",
    "# 5. Run inference\n",
    "ort_outputs = session.run(None, ort_inputs)\n",
    "# ort_outputs is a list; the first element is your model's output logits\n",
    "logits = ort_outputs[0]  # shape: [1, 2]\n",
    "\n",
    "# 6. Convert logits to probabilities and get predicted class\n",
    "#    Wrap logits with torch.tensor() just to use PyTorch’s softmax & argmax\n",
    "logits_tensor = torch.tensor(logits)\n",
    "probs = torch.softmax(logits_tensor, dim=1)\n",
    "pred_label = torch.argmax(probs, dim=1)\n",
    "\n",
    "print(\"ONNX Output logits:\", logits)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted class:\", pred_label.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"image_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected batches per epoch: 82\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expected batches per epoch: {len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qualcomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
